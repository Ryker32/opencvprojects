{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67201df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to grab frame.\n",
      "Failed to grab frame.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the Haar cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # or try 1, 2, etc.\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Cannot open camera.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        print(\"❌ Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Live Camera\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect face(s)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw face box\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Focus on upper half of face (eye region)\n",
    "        roi_gray = gray[y:y + h//2, x:x + w]\n",
    "        _, thresh = cv2.threshold(roi_gray, 50, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if 50 < area < 200:\n",
    "                x2, y2, w2, h2 = cv2.boundingRect(cnt)\n",
    "                cv2.rectangle(frame, (x + x2, y + y2), (x + x2 + w2, y + y2 + h2), (255, 0, 0), 1)\n",
    "\n",
    "    cv2.imshow('Face + Eye Region Contours', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8cd91c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     12\u001b[39m gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m faces = \u001b[43mface_cascade\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[32m     16\u001b[39m     face_roi = frame[y:y+h, x:x+w]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Convert to HSV for skin detection\n",
    "        hsv = cv2.cvtColor(face_roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Skin color range in HSV (tweak as needed for your lighting)\n",
    "        lower_skin = np.array([0, 30, 60], dtype=np.uint8)\n",
    "        upper_skin = np.array([20, 150, 255], dtype=np.uint8)\n",
    "        skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "\n",
    "        # Optional: smooth and fill holes\n",
    "        skin_mask = cv2.erode(skin_mask, None, iterations=2)\n",
    "        skin_mask = cv2.dilate(skin_mask, None, iterations=2)\n",
    "        skin_mask = cv2.GaussianBlur(skin_mask, (3, 3), 0)\n",
    "\n",
    "        # Find largest contour (likely the face region)\n",
    "        contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            largest = max(contours, key=cv2.contourArea)\n",
    "            cv2.drawContours(face_roi, [largest], -1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Face Shape Contour\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c4b173",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     15\u001b[39m gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m faces = \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[32m     19\u001b[39m     landmarks = predictor(gray, face)\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "# Load detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/ryker/CNN-Notebook/shape_predictor_68_face_landmarks.dat\")\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        points = []\n",
    "        for n in range(0, 68):  # All landmark points\n",
    "            x = landmarks.part(n).x\n",
    "            y = landmarks.part(n).y\n",
    "            points.append((x, y))\n",
    "            cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)\n",
    "\n",
    "        # Optional: Draw a convex hull around outer face contour\n",
    "        hull_points = np.array(points[0:17])  # Jawline only\n",
    "        cv2.polylines(frame, [hull_points], isClosed=False, color=(255, 0, 0), thickness=2)\n",
    "\n",
    "    cv2.imshow(\"Landmark Face Shape\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c9c5979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n",
      "gray dtype: uint8 shape: (480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "# Load models\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/ryker/CNN-Notebook/shape_predictor_68_face_landmarks.dat\")\n",
    "connections = [\n",
    "    # Jawline\n",
    "    *[(i, i+1) for i in range(0, 16)],\n",
    "    # Left eyebrow\n",
    "    *[(i, i+1) for i in range(17, 21)],\n",
    "    # Right eyebrow\n",
    "    *[(i, i+1) for i in range(22, 26)],\n",
    "    # Nose bridge\n",
    "    *[(i, i+1) for i in range(27, 30)],\n",
    "    # Nose bottom\n",
    "    *[(i, i+1) for i in range(30, 35)],\n",
    "    (35, 30),\n",
    "    # Left eye\n",
    "    *[(36,37), (37,38), (38,39), (39,36)],\n",
    "    *[(36,41), (41,40), (40,39)],\n",
    "    # Right eye\n",
    "    *[(42,43), (43,44), (44,45), (45,42)],\n",
    "    *[(42,47), (47,46), (46,45)],\n",
    "    # Outer lips\n",
    "    *[(i, i+1) for i in range(48, 59)],\n",
    "    (59, 48),\n",
    "    # Inner lips\n",
    "    *[(i, i+1) for i in range(60, 67)],\n",
    "    (67, 60)\n",
    "]\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    print(\"cant open camera.\")\n",
    "    exit()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        print(\"Failed to fetch the frame\")\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    print(\"gray dtype:\", gray.dtype, \"shape:\", gray.shape)  # Debug print\n",
    "\n",
    "    if gray.dtype != np.uint8:\n",
    "        gray = gray.astype(np.uint8)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        points = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(68)]\n",
    "\n",
    "        for x, y in points:\n",
    "            cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)\n",
    "\n",
    "        for i, j in connections:\n",
    "            pt1 = points[i]\n",
    "            pt2 = points[j]\n",
    "            cv2.line(frame, pt1, pt2, (255, 255, 0), 1)\n",
    "            #cv2.putText(frame, f\"{i}\", (pt1[0], pt1[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Geometric Face Map\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27: # (this is the escape key)\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "409f1724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    print(\"cant open camera.\")\n",
    "    exit()\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        print(\"Failed to fetch the frame\")\n",
    "        break\n",
    "    green = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    if green.dtype != np.uint8:\n",
    "        green = green.astype(np.uint8)\n",
    "    tennis_ball = cv2.inRange(green, np.array([25, 75, 85]), np.array([50, 220, 255]))\n",
    "    tennis_ball = cv2.GaussianBlur(tennis_ball, (5, 5), 0)\n",
    "    tennis_ball = cv2.morphologyEx(tennis_ball, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
    "    contours, _ = cv2.findContours(tennis_ball, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if 1000 < area < 10000:\n",
    "            (x,y),radius = cv2.minEnclosingCircle(cnt)\n",
    "            if radius < 10:\n",
    "                continue\n",
    "            center = (int(x),int(y))\n",
    "            cv2.circle(frame, center, 2, (0, 0, 255), 3)\n",
    "            x, y = int(x), int(y)\n",
    "            radius = int(radius)\n",
    "            cv2.circle(frame, center, radius, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, \"Tennis Ball\", (int(x-radius), int(y-radius)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Tennis Ball Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to exit\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abf83902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gray dtype: uint8 shape: (480, 640)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mgray dtype:\u001b[39m\u001b[33m\"\u001b[39m, gray.dtype, \u001b[33m\"\u001b[39m\u001b[33mshape:\u001b[39m\u001b[33m\"\u001b[39m, gray.shape)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m faces = \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[32m     15\u001b[39m     cv2.rectangle(frame, (face.left(), face.top()), (face.right(), face.bottom()), (\u001b[32m0\u001b[39m,\u001b[32m255\u001b[39m,\u001b[32m0\u001b[39m), \u001b[32m2\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    print(\"gray dtype:\", gray.dtype, \"shape:\", gray.shape)\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        cv2.rectangle(frame, (face.left(), face.top()), (face.right(), face.bottom()), (0,255,0), 2)\n",
    "    cv2.imshow(\"dlib test\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a15536",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     exit()\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret \u001b[38;5;129;01mor\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFailed to fetch the frame\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load OpenCV's face cascade (comes with OpenCV installation)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# If you have the dlib landmark file, we can try to use it once dlib is fixed\n",
    "# For now, we'll just do face detection\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    print(\"Can't open camera.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        print(\"Failed to fetch the frame\")\n",
    "        break\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    # Draw rectangles around faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Draw a simple face outline (not as detailed as dlib landmarks)\n",
    "        center = (x + w//2, y + h//2)\n",
    "        cv2.circle(frame, center, 2, (0, 255, 0), -1)\n",
    "        \n",
    "        # Draw some basic points\n",
    "        # Eyes (approximate positions)\n",
    "        left_eye = (x + w//4, y + h//3)\n",
    "        right_eye = (x + 3*w//4, y + h//3)\n",
    "        cv2.circle(frame, left_eye, 3, (0, 255, 0), -1)\n",
    "        cv2.circle(frame, right_eye, 3, (0, 255, 0), -1)\n",
    "        \n",
    "        # Nose\n",
    "        nose = (x + w//2, y + h//2)\n",
    "        cv2.circle(frame, nose, 2, (0, 255, 0), -1)\n",
    "        \n",
    "        # Mouth\n",
    "        mouth = (x + w//2, y + 2*h//3)\n",
    "        cv2.circle(frame, mouth, 3, (0, 255, 0), -1)\n",
    "\n",
    "    cv2.imshow(\"Face Detection (OpenCV)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d58b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret: True frame type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Cannot open camera.\")\n",
    "else:\n",
    "    ret, frame = cap.read()\n",
    "    print(\"ret:\", ret, \"frame type:\", type(frame))\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468f9873",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m height, width = frame.shape[:\u001b[32m2\u001b[39m]\n\u001b[32m     36\u001b[39m gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m faces = \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[32m     40\u001b[39m     landmarks = predictor(gray, face)\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"C:/Users/ryker/CNN-Notebook/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# make face hav etriangles\n",
    "def get_delaunay_triangles(rect, points):\n",
    "    subdiv = cv2.Subdiv2D(rect)\n",
    "    for p in points:\n",
    "        subdiv.insert(p)   \n",
    "    triangle_list = subdiv.getTriangleList()\n",
    "    triangles = []\n",
    "\n",
    "    for t in triangle_list:\n",
    "        pts = [(int(t[0]), int(t[1])), (int(t[2]), int(t[3])), (int(t[4]), int(t[5]))]\n",
    "        idx = []\n",
    "        for pt in pts:\n",
    "            for i, p in enumerate(points):\n",
    "                if abs(pt[0] - p[0]) < 2 and abs(pt[1] - p[1]) < 2:\n",
    "                    idx.append(i)\n",
    "        if len(idx) == 3:\n",
    "            triangles.append(tuple(idx))\n",
    "    return triangles\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    height, width = frame.shape[:2]\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        points = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(68)]\n",
    "\n",
    "        rect = (0, 0, width, height)\n",
    "        triangles = get_delaunay_triangles(rect, points)\n",
    "        for tri in triangles:\n",
    "            pt1 = points[tri[0]]\n",
    "            pt2 = points[tri[1]]\n",
    "            pt3 = points[tri[2]]\n",
    "            cv2.line(frame, pt1, pt2, (0, 255, 255), 1)\n",
    "            cv2.line(frame, pt2, pt3, (0, 255, 255), 1)\n",
    "            cv2.line(frame, pt3, pt1, (0, 255, 255), 1)\n",
    "        #draw the points\n",
    "        for x, y in points:\n",
    "            cv2.circle(frame, (x, y), 1, (0, 100, 255), -1)\n",
    "\n",
    "    cv2.imshow(\"Face Mesh Grid (Delaunay)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36f1350",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m frame = cv2.flip(frame, \u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m result = \u001b[43mhands\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.multi_hand_landmarks:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hand_landmarks \u001b[38;5;129;01min\u001b[39;00m result.multi_hand_landmarks:\n\u001b[32m     25\u001b[39m         \u001b[38;5;66;03m# Draw landmarks and connections\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[39m, in \u001b[36mHands.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    133\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[33;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip for natural selfie view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw landmarks and connections\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2),\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=3)\n",
    "            )\n",
    "\n",
    "            # Optional: Print fingertip points\n",
    "            fingertips = [4, 8, 12, 16, 20]  # Thumb, Index, Middle, Ring, Pinky tips\n",
    "            h, w, _ = frame.shape\n",
    "            for i in fingertips:\n",
    "                x = int(hand_landmarks.landmark[i].x * w)\n",
    "                y = int(hand_landmarks.landmark[i].y * h)\n",
    "                cv2.circle(frame, (x, y), 6, (0, 0, 255), -1)\n",
    "                cv2.putText(frame, f'{i}', (x+5, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 1)\n",
    "\n",
    "    cv2.imshow(\"Hand Tracker\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b336966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe modules\n",
    "mp_face = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Setup\n",
    "face_mesh = mp_face.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.7)\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)\n",
    "\n",
    "# Drawing specs\n",
    "face_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=(255, 0, 0))\n",
    "face_conn_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=(0, 255, 0))\n",
    "\n",
    "hand_spec = mp_drawing.DrawingSpec(thickness=2, circle_radius=3, color=(0, 255, 255))\n",
    "hand_conn_spec = mp_drawing.DrawingSpec(thickness=2, circle_radius=2, color=(255, 0, 255))\n",
    "\n",
    "# Camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    face_results = face_mesh.process(rgb)\n",
    "    hand_results = hands.process(rgb)\n",
    "\n",
    "    # Draw face mesh\n",
    "    if face_results.multi_face_landmarks:\n",
    "        for face_landmarks in face_results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face.FACEMESH_CONTOURS,  # try FACEMESH_TESSELATION for full grid\n",
    "                landmark_drawing_spec=face_spec,\n",
    "                connection_drawing_spec=face_conn_spec\n",
    "            )\n",
    "\n",
    "    # Draw hand keypoints\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                hand_spec,\n",
    "                hand_conn_spec\n",
    "            )\n",
    "\n",
    "    cv2.imshow(\"Face and Hand Tracker\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d1961",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4027268105.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip uninstall opencv-python\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
